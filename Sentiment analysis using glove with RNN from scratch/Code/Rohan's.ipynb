{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "dNxYDJ9IP7Pm",
    "outputId": "a9fd3dcf-1348-4181-eca9-9c7bf9ba7fad"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Aritra\n",
      "[nltk_data]     Banerjee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Aritra\n",
      "[nltk_data]     Banerjee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ctai82rr_NJ9"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import string \n",
    "import cupy as np\n",
    "import pandas as pd\n",
    "import time as time\n",
    "from tqdm import tqdm\n",
    "from numpy.random import randn\n",
    "import glob\n",
    "#from collections import Counter\n",
    "#from collections import defaultdict\n",
    "import operator\n",
    "from nltk.corpus import stopwords\n",
    "#from numpy.random import randn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tmny-gJ2P7P3"
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import WordNetLemmatizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wPzKb84W1kMZ"
   },
   "source": [
    "DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4OVan3Yh1UbS"
   },
   "outputs": [],
   "source": [
    "# import pandas as pd \n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# df=pd.read_csv('Tweets.csv')\n",
    "# map_dic = {'neutral':0,'positive':1,'negative':2}\n",
    "# df['label'] = df['airline_sentiment'].map(map_dic)\n",
    "# trainingSet, testSet = train_test_split(df, test_size=0.2)\n",
    "# train_data = pd.Series(trainingSet.label.values,index=trainingSet.text).to_dict()\n",
    "# test_data = pd.Series(testSet.label.values,index=testSet.text).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o9FtNmJeP7QH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kMM4nabGP7QO"
   },
   "source": [
    "### Working Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ireZo2NlP7QP"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# os.chdir(\"/home/rohan/CMI/SEM_4/NLP/Assignment_3/Data/aclImdb_v1/aclImdb/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YBMiRXLKP_oG"
   },
   "outputs": [],
   "source": [
    "# train =pd.read_csv(\"/content/drive/My Drive/Data/NLP/IMDB_csv/train.csv\")\n",
    "# test =pd.read_csv(\"/content/drive/My Drive/Data/NLP/IMDB_csv/test.csv\")\n",
    "\n",
    "# train.label = pd.Categorical(train.label)\n",
    "# train['code'] = train.label.cat.codes\n",
    "\n",
    "# test.label = pd.Categorical(test.label)\n",
    "# test['code'] = test.label.cat.codes\n",
    "\n",
    "# train_text=list(train.text)\n",
    "# train_label= list(train.code)\n",
    "\n",
    "# test_text=list(test.text)\n",
    "# test_label= list(test.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6NrtrJjTSx_m"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QBQT5W7-P7QY"
   },
   "source": [
    "### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kGhFeHF9P7QZ"
   },
   "outputs": [],
   "source": [
    "train_text=[]\n",
    "train_label=[]\n",
    "\n",
    "for file in glob.glob(\"D:/Academics/CMI/MSC_4th_sem/NLP/My_work/Assignment_3/Data/aclImdb/train/pos/*\"):\n",
    "    f = open(file, encoding='utf-8').read()\n",
    "    train_text.append(f)\n",
    "    train_label.append(1)\n",
    "\n",
    "for file in glob.glob(\"D:/Academics/CMI/MSC_4th_sem/NLP/My_work/Assignment_3/Data/aclImdb/train/neg/*\"):\n",
    "    f = open(file, encoding='utf-8').read()\n",
    "    train_text.append(f)\n",
    "    train_label.append(0)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lS2KwvQJP7Qo"
   },
   "source": [
    "### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SvmC9LnTP7Qp"
   },
   "outputs": [],
   "source": [
    "test_text=[]\n",
    "test_label=[] \n",
    "\n",
    "for file in glob.glob(\"D:/Academics/CMI/MSC_4th_sem/NLP/My_work/Assignment_3/Data/aclImdb/test/pos/*\"):\n",
    "    f = open(file, encoding='utf-8').read()\n",
    "    test_text.append(f)\n",
    "    test_label.append(1)\n",
    "\n",
    "for file in glob.glob(\"D:/Academics/CMI/MSC_4th_sem/NLP/My_work/Assignment_3/Data/aclImdb/test/neg/*\"):\n",
    "    f = open(file, encoding='utf-8').read()\n",
    "    test_text.append(f)\n",
    "    test_label.append(0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0lAAdDVn0i5-"
   },
   "source": [
    "Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fpSu0CxZkddi",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Create the vocabulary.\n",
    "# vocab = list(set([w for text in train_data.keys() for w in text.split(' ')]))\n",
    "# #vocab.append(\"OTHERS\")\n",
    "# vocab_size = len(vocab)\n",
    "# print('%d unique words found' % vocab_size) # 18 unique words found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8rHDL079P7Q3"
   },
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4d8Pu-xKP7Q4"
   },
   "outputs": [],
   "source": [
    "def remove_digits(text): \n",
    "    result = re.sub(r'\\d+', '', text) \n",
    "    return result \n",
    "\n",
    "# remove punctuation \n",
    "def remove_punctuation(text): \n",
    "    translator = str.maketrans('', '', string.punctuation) \n",
    "    return text.translate(translator) \n",
    "\n",
    "# remove whitespace from text \n",
    "def remove_extra_space(text): \n",
    "    return  \" \".join(text.split()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tHywPZ6YP7Q_"
   },
   "outputs": [],
   "source": [
    "def part_preprocessing(text):\n",
    "    text = text.lower()\n",
    "    text = remove_digits(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_extra_space(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OKylOdeYP7RK"
   },
   "outputs": [],
   "source": [
    "for i in range(len(train_text)):\n",
    "    train_text[i] = part_preprocessing(train_text[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V9NzwipDP7RT"
   },
   "outputs": [],
   "source": [
    "for i in range(len(test_text)):\n",
    "    test_text[i] = part_preprocessing(test_text[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "hGGkOnLRP7RZ",
    "outputId": "e50a4d96-efb3-4153-9a7d-4487d0bce1a5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bromwell high is a cartoon comedy it ran at the same time as some other programs about school life such as teachers my years in the teaching profession lead me to believe that bromwell highs satire is much closer to reality than is teachers the scramble to survive financially the insightful students who can see right through their pathetic teachers pomp the pettiness of the whole situation all remind me of the schools i knew and their students when i saw the episode in which a student repeatedly tried to burn down the school i immediately recalled at high a classic line inspector im here to sack one of your teachers student welcome to bromwell high i expect that many adults of my age think that bromwell high is far fetched what a pity that it isnt'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rfdB7eAvP7Rg"
   },
   "source": [
    "#### Convert Data in required format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l2Ep4wrgP7Rh",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data = {}\n",
    "\n",
    "for a, b in zip(train_text, train_label):\n",
    "    train_data[a]=  b\n",
    "\n",
    "test_data = {}\n",
    "\n",
    "for a, b in zip(test_text, test_label):\n",
    "    test_data[a]=  b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BBzfH4p6P7Rr"
   },
   "source": [
    "#### No of Data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "0rVE8QzxP7Rs",
    "outputId": "e2a9b9de-7d4a-4291-d8f9-475902e5b2ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of Total Training Data points: 24903\n",
      "No of Total Test Data points: 24800\n"
     ]
    }
   ],
   "source": [
    "print(\"No of Total Training Data points:\",len(train_data.keys()))\n",
    "print(\"No of Total Test Data points:\",len(test_data.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y9lReDj1P7Ry"
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer() \n",
    "lemmatizer = WordNetLemmatizer() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J1ZKJqYZP7R5"
   },
   "source": [
    "#### Find frequencies and document frequencies of the unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yG52OtJdP7R6"
   },
   "outputs": [],
   "source": [
    "vocab_raw = []\n",
    "count= {}\n",
    "df = {}\n",
    "for text in train_data.keys():\n",
    "    \n",
    "    for word in text.split(\" \"):\n",
    "        word = stemmer.stem(word)\n",
    "        word = lemmatizer.lemmatize(word, pos ='v')\n",
    "        vocab_raw.append(word)\n",
    "        try:\n",
    "            count[word] += 1\n",
    "        except:\n",
    "            count[word] = 1\n",
    "            try:\n",
    "                df[word] = 1\n",
    "            except:\n",
    "                df[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "LC6JH0y9P7R_",
    "outputId": "e3fe4cba-5688-4820-bdf2-d972a5dd5595"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5778645"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(count.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wQR-BTRfP7SP"
   },
   "source": [
    "#### TF-IDF values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tvx8Gb27P7SQ"
   },
   "outputs": [],
   "source": [
    "tf_idf_vals = []\n",
    "\n",
    "for word in count.keys():\n",
    "    \n",
    "    N = len(train_data)\n",
    "    tf = count[word] / sum(count.values())\n",
    "    idf =  np.log(N/df[word])\n",
    "    tf_idf = tf*idf\n",
    "    tf_idf_vals.append((word,tf_idf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HsdgZDK8P7SX"
   },
   "outputs": [],
   "source": [
    "#words = re.split(r'[;,?!\\s]\\s*', text)\n",
    "#vocab_raw = [w for text in train_data.keys() for w in text.split(\" \"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7iPm_I5XP7Sc"
   },
   "outputs": [],
   "source": [
    "# words =list(Counter(vocab_raw).keys())# equals to list(set(words))\n",
    "# counts =list(Counter(vocab_raw).values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "ijCQXgn5P7Si",
    "outputId": "ce0a34a0-b691-4bcb-988c-1c47bad9a7fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words: 5778645\n",
      "Total number of unique words: 89835\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of words:\",len(vocab_raw))\n",
    "print(\"Total number of unique words:\",len(tf_idf_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AbLxMeidP7Ss"
   },
   "outputs": [],
   "source": [
    "#vocab_list = list(zip(words, counts))\n",
    "vocab_list = tf_idf_vals\n",
    "vocab_list.sort(key=operator.itemgetter(1),reverse =True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "utzGAjeQP7Sy"
   },
   "source": [
    "#### Filterd Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tpdTLbalP7Sz"
   },
   "outputs": [],
   "source": [
    "vocab = [vocab_list[i][0] for i in range(len(vocab_list))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q9prMY21P7S5"
   },
   "source": [
    "#### Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X_hJ583SP7S6"
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens_list):\n",
    "    all_stopwords = stopwords.words('english')\n",
    "    all_stopwords.append('')\n",
    "    all_stopwords.append('br')\n",
    "    \n",
    "    out = [word for word in tokens_list if not word in all_stopwords]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "weDBGeH7P7TA"
   },
   "outputs": [],
   "source": [
    "filtered_vocab = remove_stopwords(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HcfjevnoP7TJ",
    "outputId": "580d5e45-9136-4f50-c3d6-e52be2b73312"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the vocabulary: 89725\n"
     ]
    }
   ],
   "source": [
    "#filtered_vocab = filtered_vocab\n",
    "vocab_size = len(filtered_vocab)\n",
    "print(\"Length of the vocabulary:\",vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "eDSQUDlQP7TQ",
    "outputId": "19680096-ba71-453a-9467-3a88c419a60a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the working vocabulary: 2000\n"
     ]
    }
   ],
   "source": [
    "filtered_vocab = filtered_vocab[:1999]\n",
    "filtered_vocab.append(\"OTHERS\")\n",
    "vocab_size = len(filtered_vocab)\n",
    "print(\"Length of the working vocabulary:\",vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "iSgivvIXuLe-",
    "outputId": "f4d3bb6a-69c5-4b92-a18b-5c1613d1c364"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word corresponding to 5 th index: one\n"
     ]
    }
   ],
   "source": [
    "# Assign indices to each word.\n",
    "word_to_idx = { w: i for i, w in enumerate(filtered_vocab) }\n",
    "idx_to_word = { i: w for i, w in enumerate(filtered_vocab) }\n",
    "#print(word_to_idx['thank']) # 16 (this may change)\n",
    "print(\"Word corresponding to 5 th index:\",idx_to_word[5]) # sad (this may change)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SPNfGIjyP7Td"
   },
   "source": [
    "### Create Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BXJWu-JKuLuw"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def encoded_inputs(text):\n",
    "  '''\n",
    "  Returns an array of one-hot vectors representing the words\n",
    "  in the input text string.\n",
    "  - text is a string\n",
    "  - Each one-hot vector has shape (vocab_size, 1)\n",
    "  '''\n",
    "  inputs = []\n",
    "  text = part_preprocessing(text)\n",
    "  words = text.split(' ')\n",
    "  words = remove_stopwords(words)\n",
    "\n",
    "  for word in words:\n",
    "    word = stemmer.stem(word)\n",
    "    word = lemmatizer.lemmatize(word, pos ='v')\n",
    "    encoded_word = np.zeros((vocab_size, 1))\n",
    "    if word not in word_to_idx.keys():\n",
    "      encoded_word[word_to_idx[\"OTHERS\"]] = 1\n",
    "    else:  \n",
    "      encoded_word[word_to_idx[word]] = 1\n",
    "    inputs.append(encoded_word)\n",
    "  return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JXeVZDm3P7Tm"
   },
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cmj9ATFEuL7o"
   },
   "outputs": [],
   "source": [
    "class Recurrent_NN:\n",
    "\n",
    "  def __init__(self, input_size,  hidden_size ,output_size, learning_rate= 0.001):\n",
    "        \n",
    "    # Initialize Weights and biases\n",
    "\n",
    "    ## Weights corresponding to input and hidden layer\n",
    "    self.Wxh = randn(hidden_size, input_size) / 1000\n",
    "\n",
    "    ## Weights corresponding to two hidden layers\n",
    "    self.Whh = randn(hidden_size, hidden_size) / 1000\n",
    "\n",
    "    ## Weights corresponding to ouput and hidden layer\n",
    "    self.Why = randn(output_size, hidden_size) / 1000\n",
    "\n",
    "    ## Biases corresponding to hidden layer\n",
    "    self.bh = np.zeros((hidden_size, 1))\n",
    "\n",
    "    ## Biases corresponding to output layer\n",
    "    self.by = np.zeros((output_size, 1))\n",
    "\n",
    "    ## Learning rate\n",
    "    self.learning_rate = learning_rate\n",
    "  \n",
    "  ## Activation Functions\n",
    "\n",
    "  ## Tanh activation\n",
    "\n",
    "  def tanh(self,x):\n",
    "    return np.tanh(x)\n",
    "  \n",
    "  ## Softmax Activation\n",
    "\n",
    "  def softmax(self,x):\n",
    "    return np.exp(x) / sum(np.exp(x))\n",
    "\n",
    "  ## Update parameter using gradient descent algorithm\n",
    "\n",
    "  def update_param(self, param_grad_pair):\n",
    "     x =param_grad_pair[0]\n",
    "     d_x = param_grad_pair[1]\n",
    "     x -= self.learning_rate * d_x\n",
    "     return x\n",
    "\n",
    "  ## To overcome exploding gradient problem in backprop\n",
    "\n",
    "  def not_explode_grad(self,x):\n",
    "     x = np.clip(x, -1, 1)\n",
    "     return x\n",
    "    \n",
    "  ## Compute loss in the forward prop\n",
    "\n",
    "  def calculate_loss(self,probs,target):\n",
    "     loss = - np.log(probs[target])\n",
    "     return loss\n",
    "\n",
    "  ## Forward Propagation\n",
    "\n",
    "  def forward_prop(self, inputs,target):\n",
    "\n",
    "    ## Inputs and targets\n",
    "    self.inputs = inputs\n",
    "    self.target = int(target)\n",
    "\n",
    "    ## Store h values in different time steps (Memory of RNN)\n",
    "    self.h_values = {}\n",
    "\n",
    "    ## Initialize the hidden node values \n",
    "    h = np.zeros((self.Whh.shape[0], 1))\n",
    "    self.h_values[0] = h\n",
    "    \n",
    "    for i, x in enumerate(inputs):\n",
    "      \n",
    "      ## Previous hidden layer values is being used here\n",
    "      Z = self.Wxh @ x + self.Whh @ h + self.bh\n",
    "\n",
    "      ## Tanh activation on hidden layer\n",
    "      h = self.tanh(Z)\n",
    "\n",
    "      ## Store the current h for next time step\n",
    "      self.h_values[i + 1] = h\n",
    "\n",
    "    ## Compute output in the final time step\n",
    "    y = self.Why @ h + self.by\n",
    "    \n",
    "    ## Softmax for probabilities\n",
    "    probs = self.softmax(y)\n",
    "    self.probs = probs\n",
    "\n",
    "    ## Calculate Loss\n",
    "    loss = self.calculate_loss(probs,target)\n",
    "    \n",
    "    return(y, h, probs, loss)\n",
    "\n",
    "  \n",
    "  ## Backpropagation    \n",
    "\n",
    "  def BPTT(self):\n",
    "    \n",
    "    ## Gradient of loss w.r.t y\n",
    "    d_y = self.probs\n",
    "    d_y[self.target] -= 1\n",
    "    \n",
    "    # Initialize the gradients of loss w.r.t the paramters\n",
    "    d_Whh = np.zeros(self.Whh.shape)\n",
    "    d_Wxh = np.zeros(self.Wxh.shape)\n",
    "    d_bh = np.zeros(self.bh.shape)\n",
    "\n",
    "    ## No of inputs for a input data\n",
    "    N = len(self.inputs)\n",
    "\n",
    "\n",
    "    # Following gradient depends only on ouput and last time step hidden values\n",
    "    d_Why = d_y @ self.h_values[N].T\n",
    "    d_by = d_y\n",
    "\n",
    "    # Gradient of loss w.r.t last time step h values\n",
    "    d_h = self.Why.T @ d_y\n",
    "\n",
    "    ## Backpropagate through time.\n",
    "    for t in reversed(range(N-1,-1,-1)):\n",
    "\n",
    "      # Derivative of tanh(x) w.r.t x is (1- tanh(x))^2\n",
    "      ## Need the following value in computation of gradients\n",
    "      temp = ((1 - self.h_values[t + 1] ** 2) * d_h)\n",
    "\n",
    "      # Gradient of loss w.r.t bh\n",
    "      d_bh += temp\n",
    "\n",
    "      # Gradient of loss w.r.t Whh\n",
    "      d_Whh += temp @ self.h_values[t].T\n",
    "\n",
    "      # Gradient of loss w.r.t Wxh\n",
    "      d_Wxh += temp @ self.inputs[t].T\n",
    "\n",
    "      # Gradient of loss w.r.t h\n",
    "      d_h = self.Whh @ temp\n",
    "\n",
    "    ## Get rid of exploding gradients.\n",
    "  \n",
    "    d_Wxh, d_Whh, d_Why, d_bh, d_by = list(map(self.not_explode_grad,[d_Wxh, d_Whh, d_Why, d_bh, d_by]))\n",
    "\n",
    "    ## Update weights and biases using gradient descent.\n",
    "\n",
    "    param_grad_pair = [(self.Whh,d_Whh),(self.Wxh,d_Wxh),(self.Why,d_Why),(self.bh,d_bh),(self.by,d_by)]\n",
    "    \n",
    "    self.Whh,self.Wxh,self.Why,self.bh,self.by = list(map(self.update_param,param_grad_pair))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dlsIpzouP7UC"
   },
   "source": [
    "### Function for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iucVp8FHHFck"
   },
   "outputs": [],
   "source": [
    "def compute_loss_accuracy(data, BPTT=True):\n",
    "\n",
    "    total_cost = 0\n",
    "    correct_pred= 0\n",
    "\n",
    "    items = list(data.items())\n",
    "    ## No of total data points\n",
    "    N = len(items)\n",
    "    ## Shuffle the data\n",
    "    random.shuffle(items)\n",
    "\n",
    "    \n",
    "\n",
    "    for x, y in items:\n",
    "\n",
    "        inputs = encoded_inputs(x)\n",
    "        true_label = int(y)\n",
    "        \n",
    "        print(inputs)\n",
    "\n",
    "#         out, h, probs, loss = model.forward_prop(inputs,true_label)\n",
    "#         total_cost += loss\n",
    "#         correct_pred += int(np.argmax(probs) == true_label)\n",
    "\n",
    "#         ## For test data we don't do backpropagation\n",
    "#         if BPTT:\n",
    "#           model.BPTT()\n",
    "    \n",
    "#     avg_loss = total_cost / N\n",
    "#     accuracy = correct_pred / N\n",
    "\n",
    "#     return(avg_loss, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "DP40A-aTTB99",
    "outputId": "13d206e2-140b-4b28-85f7-08d1867ce0ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Training Data: 24903\n",
      "Length of Test Data: 24800\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of Training Data:\",len(train_data.keys()))\n",
    "print(\"Length of Test Data:\",len(test_data.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0mV3_7g9P7UZ"
   },
   "source": [
    "### Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "p9Ji0qZLHPXP",
    "outputId": "02c79c41-335f-4b13-ea6a-5adad2a3c78f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Training Loss: [0.6933359] and Training Accuracy: 0.49411717463759386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|████████                                                                        | 1/10 [14:41<2:12:15, 881.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: [0.6931456] and Test Accuracy: 0.5016129032258064\n",
      "Time taken to complete epoch 1: 881.712171792984\n",
      "Epoch 2\n",
      "Training Loss: [0.69315242] and Training Accuracy: 0.501586154278601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|████████████████                                                                | 2/10 [29:27<1:57:43, 882.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: [0.69314239] and Test Accuracy: 0.5016129032258064\n",
      "Time taken to complete epoch 2: 885.6139600276947\n",
      "Epoch 3\n",
      "Training Loss: [0.6933117] and Training Accuracy: 0.49664699032245113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|████████████████████████                                                        | 3/10 [44:13<1:43:06, 883.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: [0.69322982] and Test Accuracy: 0.49838709677419357\n",
      "Time taken to complete epoch 3: 885.7601613998413\n",
      "Epoch 4\n",
      "Training Loss: [0.69323398] and Training Accuracy: 0.5036742561137212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████████████████████████████████                                                | 4/10 [58:57<1:28:24, 884.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: [0.69345846] and Test Accuracy: 0.5016129032258064\n",
      "Time taken to complete epoch 4: 884.7909731864929\n",
      "Epoch 5\n",
      "Training Loss: [0.69333475] and Training Accuracy: 0.49869493635304984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|███████████████████████████████████████                                       | 5/10 [1:13:42<1:13:41, 884.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: [0.69317647] and Test Accuracy: 0.49838709677419357\n",
      "Time taken to complete epoch 5: 884.6867325305939\n",
      "Epoch 6\n",
      "Training Loss: [0.69331141] and Training Accuracy: 0.49769104124001123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|████████████████████████████████████████████████                                | 6/10 [1:28:26<58:56, 884.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: [0.69324135] and Test Accuracy: 0.49838709677419357\n",
      "Time taken to complete epoch 6: 884.0564227104187\n",
      "Epoch 7\n",
      "Training Loss: [0.69333586] and Training Accuracy: 0.4943982652692447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|████████████████████████████████████████████████████████                        | 7/10 [1:43:11<44:13, 884.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: [0.69320103] and Test Accuracy: 0.49838709677419357\n",
      "Time taken to complete epoch 7: 885.0675346851349\n",
      "Epoch 8\n",
      "Training Loss: [0.69327375] and Training Accuracy: 0.49901618278922216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████████████████████████████████████████████████████████████                | 8/10 [1:57:56<29:28, 884.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: [0.6932791] and Test Accuracy: 0.5016129032258064\n",
      "Time taken to complete epoch 8: 884.5656213760376\n",
      "Epoch 9\n",
      "Training Loss: [0.69332807] and Training Accuracy: 0.4978516644580974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|████████████████████████████████████████████████████████████████████████        | 9/10 [2:12:41<14:44, 884.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: [0.69314237] and Test Accuracy: 0.5016129032258064\n",
      "Time taken to complete epoch 9: 885.289452791214\n",
      "Epoch 10\n",
      "Training Loss: [0.69321416] and Training Accuracy: 0.5032325422639843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 10/10 [2:27:25<00:00, 884.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: [0.69316694] and Test Accuracy: 0.49838709677419357\n",
      "Time taken to complete epoch 10: 884.172554731369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "input_size = vocab_size\n",
    "hidden_size = 20\n",
    "output_size = 2\n",
    "learning_rate = 0.001\n",
    "model = Recurrent_NN(input_size,hidden_size, output_size, learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in tqdm(range(10)):\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    training_loss, training_accuracy = compute_loss_accuracy(train_data)  \n",
    "    print('Epoch {}'.format(epoch + 1)) \n",
    "    print('Training Loss: {} and Training Accuracy: {}'.format(training_loss, training_accuracy))\n",
    "\n",
    "    test_loss, test_accuracy = compute_loss_accuracy(test_data, BPTT=False)\n",
    "    print('Test Loss: {} and Test Accuracy: {}'.format(test_loss, test_accuracy))\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    print(\"Time taken to complete epoch {}: {}\".format(epoch + 1, end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K7NZsNnjGVcb"
   },
   "source": [
    "# Rough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_accuracy(data, BPTT=True):\n",
    "\n",
    "    total_cost = 0\n",
    "    correct_pred= 0\n",
    "\n",
    "    items = list(data.items())\n",
    "    ## No of total data points\n",
    "    N = len(items)\n",
    "    ## Shuffle the data\n",
    "    random.shuffle(items)\n",
    "\n",
    "    \n",
    "\n",
    "    for x, y in items:\n",
    "\n",
    "        inputs = encoded_inputs(x)\n",
    "        true_label = int(y)\n",
    "        \n",
    "        print(np.array(inputs).shape)\n",
    "        break\n",
    "\n",
    "#         out, h, probs, loss = model.forward_prop(inputs,true_label)\n",
    "#         total_cost += loss\n",
    "#         correct_pred += int(np.argmax(probs) == true_label)\n",
    "\n",
    "#         ## For test data we don't do backpropagation\n",
    "#         if BPTT:\n",
    "#           model.BPTT()\n",
    "    \n",
    "#     avg_loss = total_cost / N\n",
    "#     accuracy = correct_pred / N\n",
    "\n",
    "#     return(avg_loss, accuracy)\n",
    "    return(inputs,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(78, 2000, 1)\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "input_size = vocab_size\n",
    "hidden_size = 20\n",
    "output_size = 2\n",
    "learning_rate = 0.001\n",
    "model = Recurrent_NN(input_size,hidden_size, output_size, learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in tqdm(range(10)):\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    training_loss, training_accuracy = compute_loss_accuracy(train_data)  \n",
    "    print('Epoch {}'.format(epoch + 1))\n",
    "    break\n",
    "#     print('Training Loss: {} and Training Accuracy: {}'.format(training_loss, training_accuracy))\n",
    "\n",
    "    test_loss, test_accuracy = compute_loss_accuracy(test_data, BPTT=False)\n",
    "#     print('Test Loss: {} and Test Accuracy: {}'.format(test_loss, test_accuracy))\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "#     print(\"Time taken to complete epoch {}: {}\".format(epoch + 1, end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        ...,\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]])]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "NLP_assignment_3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
