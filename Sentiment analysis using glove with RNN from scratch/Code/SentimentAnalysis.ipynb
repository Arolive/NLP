{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import time\n",
    "import nltk\n",
    "import string\n",
    "import random\n",
    "import numpy as np\n",
    "from numpy.random import randn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining all functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functons to load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dicList(keys):\n",
    "    return {key : [] for key in keys}\n",
    "def loadReview(path):\n",
    "    file = open(path, \"r\", encoding = \"utf8\")\n",
    "    review = file.read()\n",
    "    file.close()\n",
    "    return(review)\n",
    "def loadData(path):\n",
    "    data = dicList(dataLabel)\n",
    "    for Label in dataLabel:\n",
    "        pathNew = os.path.join(path, Label)\n",
    "        for review in os.listdir(pathNew):\n",
    "            reviewPath = os.path.join(pathNew, review)\n",
    "            review = loadReview(reviewPath)\n",
    "            data[Label].append(review)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove digits, punctuation and extra space\n",
    "def dataPreprocessing(text):\n",
    "    # Removs digits\n",
    "    def removeDigits(text): \n",
    "        result = re.sub(r'\\d+', '', text) \n",
    "        return result \n",
    "    # Removes punctuation \n",
    "    def removePunctuation(text): \n",
    "        translator = str.maketrans('', '', string.punctuation.replace(\"'\", \"\")) \n",
    "        return text.translate(translator) \n",
    "    # Removes whitespace from text \n",
    "    def removeSpaces(text): \n",
    "        return  \" \".join(text.split())\n",
    "    # Main function\n",
    "    if __name__==\"__main__\":\n",
    "        text = text.lower()\n",
    "        text = removeDigits(text)\n",
    "        text = removePunctuation(text)\n",
    "        text = removeSpaces(text)\n",
    "        return text\n",
    "\n",
    "def performProcessing(data):\n",
    "    for Label in dataLabel:\n",
    "        for index in range(len(data[Label])):\n",
    "            data[Label][index] = dataPreprocessing(data[Label][index])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return unique words\n",
    "def getUniquewords(data):\n",
    "    dataVocab = set()\n",
    "    for Label in dataLabel:\n",
    "        for review in data[Label]:\n",
    "            dataVocab.update(review.split())\n",
    "    return dataVocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check stoword\n",
    "def checkStopwords(word):\n",
    "    stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', \n",
    "                 'yours', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'now', 'd', 'll', 'm', 'o', 're',\n",
    "                 've', 'y', 'will', 'just', 's', 't', 'to', 'from', 'up', 'down', 'in', 'the', 'of', 'for', \n",
    "                 'if', 'or', 'a', 'an']\n",
    "    if word in stopwords:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functons to create Glove vocab and embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_glove_vocab(path):\n",
    "    vocab = {}\n",
    "    with open(path, 'r', encoding = \"utf8\") as f:\n",
    "        for line in f:\n",
    "            values = line.strip().split(' ')\n",
    "            word = values[0]\n",
    "            if word in dataVocab:\n",
    "                vector = np.asarray(values[1:], \"float32\")\n",
    "                vocab[word] = vector\n",
    "    return vocab\n",
    "def create_embedding(data):\n",
    "    embaddedDic = dicList(dataLabelEncod.values())\n",
    "    for Label in dataLabel:\n",
    "        dataCount = 0\n",
    "        for review in data[Label]:\n",
    "            emb = np.zeros((0,300))\n",
    "            wordcount = 0\n",
    "            for word in review.split():\n",
    "                if not checkStopwords(word):\n",
    "                    emb = np.concatenate((emb, gloveVocab.get(word, np.zeros((300)))), axis = None)\n",
    "                    wordcount += 1\n",
    "            emb = emb.reshape(wordcount,300)\n",
    "            embaddedDic[dataLabelEncod[Label]].append(emb)\n",
    "    return embaddedDic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions to create the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomShuffle(X, Y):\n",
    "    perm = np.random.permutation(len(X))\n",
    "    return(X[perm], Y[perm])\n",
    "# Function to create the input\n",
    "def getInputVec(data):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for Label in dataLabelEncod.values():\n",
    "        for emb in data[Label]:\n",
    "            X.append(emb)\n",
    "            Y.append(Label)\n",
    "    X, Y = np.array(X), np.array(Y)\n",
    "    X, Y = randomShuffle(X, Y)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    \"\"\"\n",
    "     Initialization\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size,  hidden_size ,output_size, learning_rate= 0.001):\n",
    "        # Initialize Weights and biases\n",
    "        ## Weights corresponding to input and hidden layer\n",
    "        self.Wxh = randn(hidden_size, input_size) / 1000\n",
    "        ## Weights corresponding to two hidden layers\n",
    "        self.Whh = randn(hidden_size, hidden_size) / 1000\n",
    "        ## Weights corresponding to ouput and hidden layer\n",
    "        self.Why = randn(output_size, hidden_size) / 1000\n",
    "        ## Biases corresponding to hidden layer\n",
    "        self.bh = np.zeros((hidden_size, 1))\n",
    "        ## Biases corresponding to output layer\n",
    "        self.by = np.zeros((output_size, 1))\n",
    "        ## Learning rate\n",
    "        self.learning_rate = learning_rate\n",
    "    \"\"\"\"\n",
    "     Activation Functions\n",
    "    \"\"\"\n",
    "    ## Tanh activation\n",
    "    def tanh(self,x):\n",
    "        return np.tanh(x)\n",
    "    ## Softmax Activation\n",
    "    def softmax(self,x):\n",
    "        return np.exp(x) / sum(np.exp(x))\n",
    "    \"\"\"\n",
    "     Gradient descent\n",
    "    \"\"\"\n",
    "    def update_param(self, param_grad_pair):\n",
    "        x =param_grad_pair[0]\n",
    "        d_x = param_grad_pair[1]\n",
    "        x -= self.learning_rate * d_x\n",
    "        return x\n",
    "    \"\"\"\n",
    "     Expleding gradient handler\n",
    "    \"\"\"\n",
    "    def not_explode_grad(self,x):\n",
    "        x = np.clip(x, -1, 1)\n",
    "        return x\n",
    "    \"\"\"\n",
    "     Loss function calculator\n",
    "    \"\"\"\n",
    "    def calculate_loss(self, probs, target):\n",
    "        loss = - np.log(probs[target])\n",
    "        return loss\n",
    "    \"\"\"\n",
    "     Forward propagation\n",
    "    \"\"\"\n",
    "    def forward_prop(self, inputs, target):\n",
    "        ## Inputs and targets\n",
    "        self.inputs = inputs\n",
    "        self.target = int(target)\n",
    "        ## Store h values in different time steps (Memory of RNN)\n",
    "        self.h_values = {}\n",
    "        ## Initialize the hidden node values \n",
    "        h = np.zeros((self.Whh.shape[0], 1))\n",
    "        self.h_values[0] = h\n",
    "        for i, x in enumerate(inputs):\n",
    "            ## Previous hidden layer values is being used here\n",
    "            Z = self.Wxh @ x + self.Whh @ h + self.bh\n",
    "            ## Tanh activation on hidden layer\n",
    "            h = self.tanh(Z)\n",
    "            ## Store the current h for next time step\n",
    "            self.h_values[i + 1] = h\n",
    "        ## Compute output in the final time step\n",
    "        y = self.Why @ h + self.by\n",
    "        ## Softmax for probabilities\n",
    "        probs = self.softmax(y)\n",
    "        self.probs = probs\n",
    "        ## Calculate Loss\n",
    "        loss = self.calculate_loss(probs, target)\n",
    "        return(y, h, probs, loss)\n",
    "    \"\"\"\n",
    "     Backpropagation\n",
    "    \"\"\"\n",
    "    def BPTT(self):\n",
    "        ## Gradient of loss w.r.t y\n",
    "        d_y = self.probs\n",
    "        d_y[self.target] -= 1\n",
    "        # Initialize the gradients of loss w.r.t the paramters\n",
    "        d_Whh = np.zeros(self.Whh.shape)\n",
    "        d_Wxh = np.zeros(self.Wxh.shape)\n",
    "        d_bh = np.zeros(self.bh.shape)\n",
    "        ## No of inputs for a input data\n",
    "        N = len(self.inputs)\n",
    "        # Following gradient depends only on ouput and last time step hidden values\n",
    "        d_Why = d_y @ self.h_values[N].T\n",
    "        d_by = d_y\n",
    "        # Gradient of loss w.r.t last time step h values\n",
    "        d_h = self.Why.T @ d_y\n",
    "        ## Backpropagate through time.\n",
    "        for t in reversed(range(N-1,-1,-1)):\n",
    "            # Derivative of tanh(x) w.r.t x is (1- tanh(x))^2\n",
    "            ## Need the following value in computation of gradients\n",
    "            temp = ((1 - self.h_values[t + 1] ** 2) * d_h)\n",
    "            # Gradient of loss w.r.t bh\n",
    "            d_bh += temp\n",
    "            # Gradient of loss w.r.t Whh\n",
    "            d_Whh += temp @ self.h_values[t].T\n",
    "            # Gradient of loss w.r.t Wxh\n",
    "            d_Wxh += temp @ self.inputs[t].T\n",
    "            # Gradient of loss w.r.t h\n",
    "            d_h = self.Whh @ temp\n",
    "        ## Get rid of exploding gradients.\n",
    "        d_Wxh, d_Whh, d_Why, d_bh, d_by = list(map(self.not_explode_grad,[d_Wxh, d_Whh, d_Why, d_bh, d_by]))\n",
    "        ## Update weights and biases using gradient descent.\n",
    "        param_grad_pair = [(self.Whh,d_Whh),(self.Wxh,d_Wxh),(self.Why,d_Why),(self.bh,d_bh),(self.by,d_by)]\n",
    "        self.Whh,self.Wxh,self.Why,self.bh,self.by = list(map(self.update_param,param_grad_pair))\n",
    "    \"\"\"    \n",
    "     Fitting model\n",
    "    \"\"\"\n",
    "    # Fitting with one epoch\n",
    "    def fit(self, X, Y, BPTT = True):\n",
    "        total_cost = 0\n",
    "        correct_pred = 0\n",
    "        ## No of total data points\n",
    "        N = len(Y)\n",
    "        for i in range(N):\n",
    "            inputs = X[i]\n",
    "            label = Y[i]\n",
    "            inputShape = inputs.shape\n",
    "            inputs = inputs.reshape(inputShape[0], inputShape[1], 1)\n",
    "            out, h, probs, loss = model.forward_prop(inputs, label)\n",
    "            total_cost += loss\n",
    "            correct_pred += int(np.argmax(probs) == label)\n",
    "            if BPTT:\n",
    "                model.BPTT()\n",
    "        avg_loss = total_cost / N\n",
    "        accuracy = correct_pred / N\n",
    "        return(avg_loss, accuracy)\n",
    "    # Defining epoch wise learning\n",
    "    def train(self, X, Y, epoch = 5):\n",
    "        for e in range(epoch):\n",
    "            loss, accuracy = model.fit(X, Y)\n",
    "            print('Epoch {}'.format(e + 1))\n",
    "            print('Loss: {} and Accuracy: {}'.format(loss[0], accuracy))\n",
    "    \"\"\"\n",
    "     Prediction\n",
    "    \"\"\"\n",
    "    def predict(self, X):\n",
    "        prediction = []\n",
    "        N = len(X)\n",
    "        for i in range(N):\n",
    "            ## Inputs\n",
    "            inputs = X[i]\n",
    "            inputShape = inputs.shape\n",
    "            inputs = inputs.reshape(inputShape[0], inputShape[1], 1)\n",
    "            ## Store h values in different time steps (Memory of RNN)\n",
    "            self.h_values = {}\n",
    "            ## Initialize the hidden node values \n",
    "            h = np.zeros((self.Whh.shape[0], 1))\n",
    "            self.h_values[0] = h\n",
    "            for i, x in enumerate(inputs):\n",
    "                ## Previous hidden layer values is being used here\n",
    "                Z = self.Wxh @ x + self.Whh @ h + self.bh\n",
    "                ## Tanh activation on hidden layer\n",
    "                h = self.tanh(Z)\n",
    "                ## Store the current h for next time step\n",
    "                self.h_values[i + 1] = h\n",
    "            ## Compute output in the final time step\n",
    "            y = self.Why @ h + self.by\n",
    "            ## Softmax for probabilities\n",
    "            probs = self.softmax(y)\n",
    "            pred = int(np.argmax(probs))\n",
    "            prediction.append(pred)\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainPath = \"D:/Academics/CMI/MSC_4th_sem/NLP/My_work/Assignment_3/Data/aclImdb/train\"\n",
    "TestPath = \"D:/Academics/CMI/MSC_4th_sem/NLP/My_work/Assignment_3/Data/aclImdb/test\"\n",
    "glovePath = \"D:/Academics/CMI/MSC_4th_sem/NLP/My_work/Assignment_3/Data/glove.840B.300d/glove.840B.300d.txt\"\n",
    "\n",
    "dataType = [\"train\", \"test\"]\n",
    "dataLabel = [\"pos\", \"neg\"]\n",
    "dataLabelEncod = {\"pos\" : 0, \"neg\" : 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = loadData(TrainPath)\n",
    "testData = loadData(TestPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = performProcessing(trainData)\n",
    "testData = performProcessing(testData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataVocab = getUniquewords(trainData)\n",
    "dataVocab.update(testData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating glove embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "gloveVocab = get_glove_vocab(glovePath)      # Loading pretrained glove model\n",
    "embaddedTrain = create_embedding(trainData)  # Building embedding for train data\n",
    "embaddedTest = create_embedding(testData)    # Building embedding for test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating input vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, trainY = getInputVec(embaddedTrain)\n",
    "testX, testY = getInputVec(embaddedTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "  Writting embedding vectors in a file\n",
    "\"\"\"\n",
    "np.save(\"D:/Academics/CMI/MSC_4th_sem/NLP/My_work/Assignment_3/Data/OutputData/trainX\", trainX) \n",
    "np.save(\"D:/Academics/CMI/MSC_4th_sem/NLP/My_work/Assignment_3/Data/OutputData/trainY\", trainY)\n",
    "np.save(\"D:/Academics/CMI/MSC_4th_sem/NLP/My_work/Assignment_3/Data/OutputData/testX\", testX)\n",
    "np.save(\"D:/Academics/CMI/MSC_4th_sem/NLP/My_work/Assignment_3/Data/OutputData/testY\", testY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "  Loading embedding vectors from file\n",
    "\"\"\"\n",
    "trainX = np.load(\"D:/Academics/CMI/MSC_4th_sem/NLP/My_work/Assignment_3/Data/OutputData/trainX.npy\", allow_pickle = True)\n",
    "trainY = np.load(\"D:/Academics/CMI/MSC_4th_sem/NLP/My_work/Assignment_3/Data/OutputData/trainY.npy\")\n",
    "testX = np.load(\"D:/Academics/CMI/MSC_4th_sem/NLP/My_work/Assignment_3/Data/OutputData/trainY.npy\", allow_pickle = True)\n",
    "testY = np.load(\"D:/Academics/CMI/MSC_4th_sem/NLP/My_work/Assignment_3/Data/OutputData/testY.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 300\n",
    "hidden_size = 25\n",
    "output_size = 2\n",
    "learning_rate = 0.001\n",
    "model = RNN(input_size, hidden_size, output_size, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Loss: 0.6933052304800627 and Accuracy: 0.49632\n",
      "Epoch 2\n",
      "Loss: 0.6904994498227014 and Accuracy: 0.53324\n",
      "Epoch 3\n",
      "Loss: 0.6801195955770556 and Accuracy: 0.561\n",
      "Epoch 4\n",
      "Loss: 0.6792281988009802 and Accuracy: 0.5612\n",
      "Epoch 5\n",
      "Loss: 0.6791135560802545 and Accuracy: 0.561\n"
     ]
    }
   ],
   "source": [
    "model.train(trainX, trainY, epoch = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing with test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.56232"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(predicted, testY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring with other hyper parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 300\n",
    "hidden_size = 5\n",
    "output_size = 2\n",
    "learning_rate = 0.001\n",
    "model2 = RNN(input_size, hidden_size, output_size, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Loss: 0.6772098003265945 and Accuracy: 0.573\n",
      "Epoch 2\n",
      "Loss: 0.6771208034356248 and Accuracy: 0.5736\n",
      "Epoch 3\n",
      "Loss: 0.6770216645948988 and Accuracy: 0.5732\n",
      "Epoch 4\n",
      "Loss: 0.6769168445064423 and Accuracy: 0.5736\n",
      "Epoch 5\n",
      "Loss: 0.6768107202754022 and Accuracy: 0.574\n"
     ]
    }
   ],
   "source": [
    "model2.train(trainX[:5000], trainY[:5000], epoch = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.50136"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = model2.predict(testX)\n",
    "accuracy_score(predicted, testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
