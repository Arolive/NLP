{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter name of the user: Aritra Banerjee\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "## Reading path of the data and user name\n",
    "Rawpath = \"..\\\\lib\"\n",
    "whatsappName = input(\"Enter name of the user: \")\n",
    "\n",
    "## Reading data\n",
    "data = \"\"\n",
    "inputPath = os.path.join(Rawpath)\n",
    "for file in os.listdir(inputPath):\n",
    "    filePath = os.path.join(inputPath, file)\n",
    "    with open(filePath, encoding = \"utf8\") as f:\n",
    "        temp = f.read()\n",
    "    data += \" \" + temp\n",
    "\n",
    "## Creaitng output folder\n",
    "outputPath = os.path.join(Rawpath, \"Output\\\\\")\n",
    "if not os.path.exists(os.path.dirname(outputPath)):\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(outputPath))\n",
    "    except OSError as exc: # Guard against race condition\n",
    "        if exc.errno != errno.EEXIST:\n",
    "            raise\n",
    "\n",
    "## Splitting data as new message as #brk#\n",
    "data = re.sub(\"\\n[0-9]*/[0-9]*/[0-9]*, [0-9]*:[0-9]* [ap][m] - \", \" #brk# \", data)\n",
    "data = data.split(\" #brk# \")[1:]\n",
    "\n",
    "## Creating in and out messages\n",
    "dataList = []\n",
    "for line in data:\n",
    "    line = line.replace(\"\\n\", \" \")\n",
    "    dataList.append(line.split(\": \"))\n",
    "    \n",
    "flag_0 = True\n",
    "flag_1 = True\n",
    "incoming = []\n",
    "outgoing = []\n",
    "temp = \"\"\n",
    "for item in dataList:\n",
    "    try:\n",
    "        name = item[0]\n",
    "        txt = item[1]\n",
    "    except:\n",
    "        continue\n",
    "    if name == whatsappName:\n",
    "        if flag_0:\n",
    "            if not flag_1:\n",
    "                incoming.append(temp)\n",
    "            temp = txt\n",
    "            flag_1 = True\n",
    "            flag_0 = False\n",
    "        else:\n",
    "            temp += \" \" + txt\n",
    "    else:\n",
    "        if flag_1:\n",
    "            if not flag_0:\n",
    "                outgoing.append(temp)\n",
    "            temp = txt\n",
    "            flag_0 = True\n",
    "            flag_1 = False\n",
    "        else:\n",
    "            temp += \" \" + txt\n",
    "            \n",
    "\n",
    "## Writting created dataset\n",
    "with open(outputPath + '\\\\incoming.txt', 'w', encoding = \"utf8\") as f:\n",
    "    for item in incoming:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "with open(outputPath + '\\\\outgoing.txt', 'w', encoding = \"utf8\") as f:\n",
    "    for item in outgoing:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import random\n",
    "import string\n",
    "import warnings\n",
    "import pickle\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove digits, punctuation and extra space\n",
    "def processed(text):\n",
    "    # Removs digits\n",
    "    def removeDigits(text): \n",
    "        result = re.sub(r'\\d+', '', text) \n",
    "        return result \n",
    "    # Removes punctuation \n",
    "    def removePunctuation(text): \n",
    "        translator = str.maketrans('', '', string.punctuation.replace(\":\", \"\").replace(\"'\",\"\")) \n",
    "        return text.translate(translator) \n",
    "    # Removes whitespace from text \n",
    "    def removeSpaces(text): \n",
    "        return  \" \".join(text.split())\n",
    "    # Main function\n",
    "    if __name__==\"__main__\":\n",
    "        text = text.replace(\"<Media omitted>\", \" \")\n",
    "        text = text.lower()\n",
    "        #text = removeDigits(text)\n",
    "        text = removePunctuation(text)\n",
    "        text = removeSpaces(text)\n",
    "        return text\n",
    "def performprocess(ListOfMsg):\n",
    "    output = []\n",
    "    for text in ListOfMsg:\n",
    "        output.append(processed(text))\n",
    "    return(output)\n",
    "\n",
    "def getWordCount(corpus):\n",
    "    WordCount = {}\n",
    "    for text in corpus:\n",
    "        for word in text.split():\n",
    "            try:\n",
    "                WordCount[word] += 1\n",
    "            except:\n",
    "                WordCount[word] = 1 \n",
    "    return WordCount\n",
    "\n",
    "def getIndexDicMap(incoming, outgoing):\n",
    "    inDic = {}\n",
    "    outDic = {}\n",
    "    for i in range(len(incoming)):\n",
    "        if incoming[i] not in inDic.keys():\n",
    "            inDic[incoming[i]] = i\n",
    "    for i in range(len(outgoing)):\n",
    "        if outgoing[i] not in outDic.keys():\n",
    "            outDic[outgoing[i]] = i\n",
    "    mappingDic = {}\n",
    "    for index in range(len(incoming)):\n",
    "        try:\n",
    "            mappingDic[inDic[incoming[index]]].append(outDic[outgoing[index]])\n",
    "        except:\n",
    "            mappingDic[inDic[incoming[index]]] = [outDic[outgoing[index]]]\n",
    "    return inDic, outDic, mappingDic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputPath = \"..\\\\lib\\\\Output\\\\\"\n",
    "outputPath = \"..\\\\lib\\\\Output\\\\vocab\\\\\"\n",
    "\n",
    "with open(inputPath + \"incoming.txt\", encoding = \"utf8\") as f:\n",
    "    incoming = f.read()\n",
    "incoming = incoming.split(\"\\n\")        \n",
    "with open(inputPath + \"outgoing.txt\", encoding = \"utf8\") as f:\n",
    "    outgoing = f.read()\n",
    "outgoing = outgoing.split(\"\\n\")\n",
    "\n",
    "inProcessed = performprocess(incoming)\n",
    "outProcessed = performprocess(outgoing)\n",
    "\n",
    "inWordCount = getWordCount(inProcessed)\n",
    "\n",
    "inDic, outDic, mappingDic = getIndexDicMap(inProcessed, outProcessed)\n",
    "outDicMap = dict([(value, key) for key, value in outDic.items()]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.dirname(outputPath)):\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(outputPath))\n",
    "    except OSError as exc: # Guard against race condition\n",
    "        if exc.errno != errno.EEXIST:\n",
    "            raise            \n",
    "            \n",
    "with open(outputPath + \"inProcessed.p\", 'wb') as f:\n",
    "    pickle.dump(inProcessed, f)\n",
    "with open(outputPath + \"outProcessed.p\", 'wb') as f:\n",
    "    pickle.dump(outProcessed, f)\n",
    "with open(outputPath + \"inWordCount.p\", 'wb') as f:\n",
    "    pickle.dump(inWordCount, f)\n",
    "with open(outputPath + \"inDic.p\", 'wb') as f:\n",
    "    pickle.dump(inDic, f)\n",
    "with open(outputPath + \"mappingDic.p\", 'wb') as f:\n",
    "    pickle.dump(mappingDic, f)\n",
    "with open(outputPath + \"outDicMap.p\", 'wb') as f:\n",
    "    pickle.dump(outDicMap, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import scipy\n",
    "import random\n",
    "import pickle \n",
    "import warnings\n",
    "import BuildVocab\n",
    "import numpy as np\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self):\n",
    "        self.SIZE = 100\n",
    "        self.WINDOW = 5\n",
    "        self.ALPHA = 0.001\n",
    "        self.Method = 1\n",
    "\n",
    "        self.inputPath = \"..\\\\lib\\\\Output\\\\vocab\\\\\"\n",
    "        self.outputPath = \"..\\\\lib\\\\Output\\\\weights\\\\\"\n",
    "\n",
    "        with open(self.inputPath + \"inProcessed.p\", 'rb') as f:\n",
    "            self.inProcessed = pickle.load(f)\n",
    "#         with open(self.inputPath + \"outProcessed.p\", 'rb') as f:\n",
    "#             self.outProcessed = pickle.load(f)\n",
    "        with open(self.inputPath + \"inWordCount.p\", 'rb') as f:\n",
    "            self.inWordCount = pickle.load(f)\n",
    "        with open(self.inputPath + \"inDic.p\", 'rb') as f:\n",
    "            self.inDic = pickle.load(f)\n",
    "        with open(self.inputPath + \"mappingDic.p\", 'rb') as f:\n",
    "            self.mappingDic = pickle.load(f)\n",
    "        with open(self.inputPath + \"outDicMap.p\", 'rb') as f:\n",
    "            self.outDicMap = pickle.load(f)\n",
    "\n",
    "        if not os.path.exists(os.path.dirname(self.outputPath)): \n",
    "            try:\n",
    "                os.makedirs(os.path.dirname(self.outputPath))\n",
    "            except OSError as exc: # Guard against race condition\n",
    "                if exc.errno != errno.EEXIST:\n",
    "                    raise\n",
    "            print(\"Model training under process...\")\n",
    "            self.model = self.trainModel(self.inProcessed)\n",
    "            self.model.wv.save_word2vec_format(self.outputPath + \"word2vecWeights.txt\", binary=False)  \n",
    "        else:\n",
    "            print(\"Trained weights found\\nLoading..\")\n",
    "            self.model = KeyedVectors.load_word2vec_format(self.outputPath + \"word2vecWeights.txt\", binary=False)\n",
    "    ## Function to training a word2vec model    \n",
    "    def trainModel(self):\n",
    "        trainData = list(i.split() for i in self.inProcessed)\n",
    "        model = Word2Vec(trainData, size = self.Size, window = self.Window, min_count = 1, workers = 4)\n",
    "        return model  \n",
    "    \n",
    "    ## Function to return the word embedding of the word, zeros if the wor is unknown\n",
    "    def Vec(self, word):\n",
    "        try:\n",
    "            return self.model[word]\n",
    "        except:\n",
    "            return np.zeros(self.SIZE)\n",
    "        \n",
    "    ## Function to return the count of the word in the corpus   \n",
    "    def Count(self, word):\n",
    "        try:\n",
    "            return self.inWordCount[word]\n",
    "        except:\n",
    "            return 0\n",
    "        \n",
    "    ## Function to return the similarity distance (cosine) between two senetnce.\n",
    "    ## 0 implies they are totally similar and 1 implies they are way apart\n",
    "    def CosSifDist(self, sentence_1, sentence_2):\n",
    "        vector_1 = np.sum([self.Vec(word)*(self.ALPHA / (self.ALPHA + self.Count(word))) for word in BuildVocab.processed(sentence_1).split()], axis = 0)\n",
    "        vector_2 = np.sum([self.Vec(word)*(self.ALPHA / (self.ALPHA + self.Count(word))) for word in BuildVocab.processed(sentence_2).split()], axis = 0)\n",
    "        cosine = scipy.spatial.distance.cosine(vector_1, vector_2)\n",
    "        #print('Word Embedding method with a cosine distance method, sentences are similar to',round((1-cosine)*100,2),'%')\n",
    "        return cosine  \n",
    "    \n",
    "    ## Function tto return the similarity distance (word mover) between two senetnce.\n",
    "    def WMDist(sentence_1, sentence_2):\n",
    "        sentence_1 = BuildVocab.processed(sentence_1)\n",
    "        sentence_2 = BuildVocab.processed(sentence_2)\n",
    "        return self.model.wmdistance(sentence_1, sentence_2)    \n",
    "    \n",
    "    ## Function to return the closest messages given a raw text\n",
    "    def getCloseMsg(self, RawText):\n",
    "        if self.Method == 1:\n",
    "            temp = np.inf\n",
    "            closestMsg = None\n",
    "            for oldmsg in self.inProcessed:\n",
    "                similarity = self.CosSifDist(RawText, oldmsg)\n",
    "                if similarity < temp:\n",
    "                    temp = similarity\n",
    "                    closestMsg = oldmsg\n",
    "        elif self.Method == 2:\n",
    "            temp = np.inf\n",
    "            closestMsg = None\n",
    "            for oldmsg in self.inProcessed:\n",
    "                similarity = self.WMDist(RawText, oldmsg)\n",
    "                if similarity < temp:\n",
    "                    temp = similarity\n",
    "                    closestMsg = oldmsg\n",
    "        return closestMsg \n",
    "    \n",
    "    ## Function to return the reply of a message\n",
    "    def getReply(self, newMsg):\n",
    "        replies = []\n",
    "        msg = self.getCloseMsg(newMsg)\n",
    "        index = self.inDic[msg]\n",
    "        repliesindex = self.mappingDic[index]\n",
    "        for i in repliesindex:\n",
    "            replies.append(self.outDicMap[i])\n",
    "        return(random.choice(replies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary found\n",
      "Loading...\n",
      "Trained weights found\n",
      "Loading..\n",
      "Weights loaded successfully\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "import utils.bot as bot\n",
    "\n",
    "model = bot.Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ðŸ¤– -> Sorry, I didnot get you..'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.getReply(\"Hola\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
